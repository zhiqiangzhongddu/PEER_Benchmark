{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:54:59.620250Z",
     "start_time": "2023-07-02T16:54:58.126377Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from torchdrug.utils import comm, pretty\n",
    "from torchdrug import data, core, utils\n",
    "from torch.utils import data as torch_data\n",
    "\n",
    "from IPython import get_ipython\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "if is_notebook():\n",
    "    sys.path.append('/home/zhiqiang/PEER_Benchmark')\n",
    "else:\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(__file__)))\n",
    "\n",
    "from peer import protbert, util, flip\n",
    "from script.run_single import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:54:59.626086Z",
     "start_time": "2023-07-02T16:54:59.623865Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# train the model, same as PEER code\n",
    "args = parse_args()\n",
    "\n",
    "args.config = '/home/zhiqiang/PEER_Benchmark/config/single_task/ESM/gb1_ESM_fix.yaml' \\\n",
    "    if is_notebook() else os.path.realpath(args.config)\n",
    "cfg = util.load_config(args.config)\n",
    "if cfg.dataset[\"class\"] != \"Fluorescence\":\n",
    "    cfg.dataset[\"split\"] = args.split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:54:59.626228Z",
     "start_time": "2023-07-02T16:54:59.624140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:54:59   Config file: /home/zhiqiang/PEER_Benchmark/config/single_task/ESM/gb1_ESM_fix.yaml\n",
      "16:54:59   {'dataset': {'atom_feature': None,\n",
      "             'bond_feature': None,\n",
      "             'class': 'GB1',\n",
      "             'path': '~/scratch/protein-datasets/',\n",
      "             'split': 'two_vs_rest',\n",
      "             'transform': {'class': 'Compose',\n",
      "                           'transforms': [{'class': 'ProteinView',\n",
      "                                           'view': 'residue'}]}},\n",
      " 'engine': {'batch_size': 32, 'gpus': [0]},\n",
      " 'eval_metric': 'spearmanr',\n",
      " 'fix_encoder': True,\n",
      " 'optimizer': {'class': 'Adam', 'lr': 5e-05},\n",
      " 'output_dir': '~/scratch/torchprotein_output/',\n",
      " 'task': {'class': 'PropertyPrediction',\n",
      "          'criterion': 'mse',\n",
      "          'metric': ['mae', 'rmse', 'spearmanr'],\n",
      "          'model': {'class': 'ESM',\n",
      "                    'model': 'ESM-1b',\n",
      "                    'path': '~/scratch/protein-model-weights/esm-model-weights/',\n",
      "                    'readout': 'mean'},\n",
      "          'normalization': False,\n",
      "          'num_mlp_layer': 2},\n",
      " 'train': {'num_epoch': 30}}\n",
      "16:54:59   Output dir: /home/zhiqiang/scratch/torchprotein_output/PropertyPrediction/GB1/ESM_2023-07-02-16-54-59\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "output_dir = util.create_working_directory(cfg)\n",
    "logger = util.get_root_logger()\n",
    "if comm.get_rank() == 0:\n",
    "    logger.warning(\"Config file: %s\" % args.config)\n",
    "    logger.warning(pprint.pformat(cfg))\n",
    "    logger.warning(\"Output dir: %s\" % output_dir)\n",
    "    shutil.copyfile(args.config, os.path.basename(args.config))\n",
    "os.chdir(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:54:59.656864Z",
     "start_time": "2023-07-02T16:54:59.644495Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:54:59.656993Z",
     "start_time": "2023-07-02T16:54:59.650745Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:54:59   Extracting /home/zhiqiang/scratch/protein-datasets/gb1/splits.zip to /home/zhiqiang/scratch/protein-datasets/gb1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/zhiqiang/scratch/protein-datasets/gb1/splits/two_vs_rest.csv: 100%|██████████| 8734/8734 [00:00<00:00, 156473.26it/s]\n",
      "Constructing proteins from sequences: 100%|██████████| 8733/8733 [00:06<00:00, 1317.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:55:06   GB1(\n",
      "  #sample: 8733\n",
      "  #task: 1\n",
      ")\n",
      "16:55:06   #train: 381, #valid: 43, #test: 8309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:55:23   Preprocess training set\n",
      "16:55:25   {'batch_size': 32,\n",
      " 'class': 'core.Engine',\n",
      " 'gpus': [0],\n",
      " 'gradient_interval': 1,\n",
      " 'log_interval': 100,\n",
      " 'logger': 'logging',\n",
      " 'num_worker': 0,\n",
      " 'optimizer': {'amsgrad': False,\n",
      "               'betas': (0.9, 0.999),\n",
      "               'capturable': False,\n",
      "               'class': 'optim.Adam',\n",
      "               'eps': 1e-08,\n",
      "               'foreach': None,\n",
      "               'lr': 5e-05,\n",
      "               'maximize': False,\n",
      "               'weight_decay': 0},\n",
      " 'scheduler': None,\n",
      " 'task': {'class': 'tasks.PropertyPrediction',\n",
      "          'criterion': 'mse',\n",
      "          'graph_construction_model': None,\n",
      "          'metric': ['mae', 'rmse', 'spearmanr'],\n",
      "          'mlp_batch_norm': False,\n",
      "          'mlp_dropout': 0,\n",
      "          'model': {'class': 'models.ESM',\n",
      "                    'model': 'ESM-1b',\n",
      "                    'path': '~/scratch/protein-model-weights/esm-model-weights/',\n",
      "                    'readout': 'mean'},\n",
      "          'normalization': False,\n",
      "          'num_class': None,\n",
      "          'num_mlp_layer': 2,\n",
      "          'task': ['target'],\n",
      "          'verbose': 0},\n",
      " 'test_set': {'class': 'dataset.Subset',\n",
      "              'dataset': {'atom_feature': None,\n",
      "                          'bond_feature': None,\n",
      "                          'class': 'datasets.GB1',\n",
      "                          'path': '~/scratch/protein-datasets/',\n",
      "                          'split': 'two_vs_rest',\n",
      "                          'transform': {'class': 'transforms.Compose',\n",
      "                                        'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                        'keys': 'graph',\n",
      "                                                        'view': 'residue'}]},\n",
      "                          'verbose': 1},\n",
      "              'indices': range(424, 8733)},\n",
      " 'train_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'datasets.GB1',\n",
      "                           'path': '~/scratch/protein-datasets/',\n",
      "                           'split': 'two_vs_rest',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                         'keys': 'graph',\n",
      "                                                         'view': 'residue'}]},\n",
      "                           'verbose': 1},\n",
      "               'indices': range(0, 381)},\n",
      " 'valid_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'datasets.GB1',\n",
      "                           'path': '~/scratch/protein-datasets/',\n",
      "                           'split': 'two_vs_rest',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                         'keys': 'graph',\n",
      "                                                         'view': 'residue'}]},\n",
      "                           'verbose': 1},\n",
      "               'indices': range(381, 424)}}\n",
      "16:55:25   Extracting /home/zhiqiang/scratch/protein-datasets/gb1/splits.zip to /home/zhiqiang/scratch/protein-datasets/gb1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/zhiqiang/scratch/protein-datasets/gb1/splits/two_vs_rest.csv: 100%|██████████| 8734/8734 [00:00<00:00, 183694.30it/s]\n",
      "Constructing proteins from sequences: 100%|██████████| 8733/8733 [00:06<00:00, 1316.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:55:31   GB1(\n",
      "  #sample: 8733\n",
      "  #task: 1\n",
      ")\n",
      "16:55:31   #train: 381, #valid: 43, #test: 8309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "solver = build_solver(cfg, logger)\n",
    "# build dataset\n",
    "_dataset = core.Configurable.load_config_dict(cfg.dataset)\n",
    "if \"test_split\" in cfg:\n",
    "    train_set, valid_set, test_set = _dataset.split(['train', 'valid', cfg.test_split])\n",
    "else:\n",
    "    train_set, valid_set, test_set = _dataset.split()\n",
    "if comm.get_rank() == 0:\n",
    "    logger.warning(_dataset)\n",
    "    logger.warning(\"#train: %d, #valid: %d, #test: %d\" % (len(train_set), len(valid_set), len(test_set)))\n",
    "\n",
    "# build task model\n",
    "if cfg.task[\"class\"] in [\"PropertyPrediction\", \"InteractionPrediction\"]:\n",
    "    cfg.task.task = _dataset.tasks\n",
    "task = core.Configurable.load_config_dict(cfg.task)\n",
    "\n",
    "# fix the pre-trained encoder if specified\n",
    "fix_encoder = cfg.get(\"fix_encoder\", False)\n",
    "fix_encoder2 = cfg.get(\"fix_encoder2\", False)\n",
    "if fix_encoder:\n",
    "    for p in task.model.parameters():\n",
    "        p.requires_grad = False\n",
    "if fix_encoder2:\n",
    "    for p in task.model2.parameters():\n",
    "        p.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:55:48.655112Z",
     "start_time": "2023-07-02T16:54:59.650826Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:55:48.655643Z",
     "start_time": "2023-07-02T16:55:48.654442Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:55:48   Preprocess training set\n",
      "16:55:49   {'batch_size': 32,\n",
      " 'class': 'core.Engine',\n",
      " 'gpus': [0],\n",
      " 'gradient_interval': 1,\n",
      " 'log_interval': 100,\n",
      " 'logger': 'logging',\n",
      " 'num_worker': 0,\n",
      " 'optimizer': {'amsgrad': False,\n",
      "               'betas': (0.9, 0.999),\n",
      "               'capturable': False,\n",
      "               'class': 'optim.Adam',\n",
      "               'eps': 1e-08,\n",
      "               'foreach': None,\n",
      "               'lr': 5e-05,\n",
      "               'maximize': False,\n",
      "               'weight_decay': 0},\n",
      " 'scheduler': None,\n",
      " 'task': {'class': 'tasks.PropertyPrediction',\n",
      "          'criterion': 'mse',\n",
      "          'graph_construction_model': None,\n",
      "          'metric': ['mae', 'rmse', 'spearmanr'],\n",
      "          'mlp_batch_norm': False,\n",
      "          'mlp_dropout': 0,\n",
      "          'model': {'class': 'models.ESM',\n",
      "                    'model': 'ESM-1b',\n",
      "                    'path': '~/scratch/protein-model-weights/esm-model-weights/',\n",
      "                    'readout': 'mean'},\n",
      "          'normalization': False,\n",
      "          'num_class': None,\n",
      "          'num_mlp_layer': 2,\n",
      "          'task': ['target'],\n",
      "          'verbose': 0},\n",
      " 'test_set': {'class': 'dataset.Subset',\n",
      "              'dataset': {'atom_feature': None,\n",
      "                          'bond_feature': None,\n",
      "                          'class': 'datasets.GB1',\n",
      "                          'path': '~/scratch/protein-datasets/',\n",
      "                          'split': 'two_vs_rest',\n",
      "                          'transform': {'class': 'transforms.Compose',\n",
      "                                        'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                        'keys': 'graph',\n",
      "                                                        'view': 'residue'}]},\n",
      "                          'verbose': 1},\n",
      "              'indices': range(424, 8733)},\n",
      " 'train_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'datasets.GB1',\n",
      "                           'path': '~/scratch/protein-datasets/',\n",
      "                           'split': 'two_vs_rest',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                         'keys': 'graph',\n",
      "                                                         'view': 'residue'}]},\n",
      "                           'verbose': 1},\n",
      "               'indices': range(0, 381)},\n",
      " 'valid_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'datasets.GB1',\n",
      "                           'path': '~/scratch/protein-datasets/',\n",
      "                           'split': 'two_vs_rest',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                         'keys': 'graph',\n",
      "                                                         'view': 'residue'}]},\n",
      "                           'verbose': 1},\n",
      "               'indices': range(381, 424)}}\n"
     ]
    }
   ],
   "source": [
    "# build solver\n",
    "cfg.optimizer.params = task.parameters()\n",
    "optimizer = core.Configurable.load_config_dict(cfg.optimizer)\n",
    "if not \"scheduler\" in cfg:\n",
    "    scheduler = None\n",
    "else:\n",
    "    cfg.scheduler.optimizer = optimizer\n",
    "    scheduler = core.Configurable.load_config_dict(cfg.scheduler)\n",
    "\n",
    "solver = core.Engine(task, train_set, valid_set, test_set, optimizer, scheduler, **cfg.engine)\n",
    "if \"lr_ratio\" in cfg:\n",
    "    cfg.optimizer.params = [\n",
    "        {'params': solver.model.model.parameters(), 'lr': cfg.optimizer.lr * cfg.lr_ratio},\n",
    "        {'params': solver.model.mlp.parameters(), 'lr': cfg.optimizer.lr}\n",
    "    ]\n",
    "    optimizer = core.Configurable.load_config_dict(cfg.optimizer)\n",
    "    solver.optimizer = optimizer\n",
    "if \"checkpoint\" in cfg:\n",
    "    solver.load(cfg.checkpoint, load_optimizer=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:55:49.336645Z",
     "start_time": "2023-07-02T16:55:48.659796Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T16:55:49.352970Z",
     "start_time": "2023-07-02T16:55:49.348940Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# solver, best_epoch = train_and_validate(cfg, solver)\n",
    "\"\"\"start solver, best_epoch = train_and_validate(cfg, solver)\"\"\"\n",
    "step = math.ceil(cfg.train.num_epoch / 10)\n",
    "best_score = float(\"-inf\")\n",
    "best_epoch = -1\n",
    "\n",
    "# if not cfg.train.num_epoch > 0:\n",
    "#     return solver, best_epoch\n",
    "\n",
    "for i in range(0, cfg.train.num_epoch, step):\n",
    "    kwargs = cfg.train.copy()\n",
    "    kwargs[\"num_epoch\"] = min(step, cfg.train.num_epoch - i)\n",
    "    solver.model.split = \"train\"\n",
    "    # solver.train(**kwargs)\n",
    "    \"\"\"start solver.train(**kwargs)\"\"\"\n",
    "    from torch import nn\n",
    "    from itertools import islice\n",
    "    batch_per_epoch = None\n",
    "    num_epoch = kwargs[\"num_epoch\"]\n",
    "    sampler = torch_data.DistributedSampler(solver.train_set, solver.world_size, solver.rank)\n",
    "    dataloader = data.DataLoader(solver.train_set, solver.batch_size, sampler=sampler, num_workers=solver.num_worker)\n",
    "    batch_per_epoch = batch_per_epoch or len(dataloader)\n",
    "    model = solver.model\n",
    "    model.split = \"train\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in solver.meter(num_epoch):\n",
    "        sampler.set_epoch(epoch)\n",
    "\n",
    "        metrics = []\n",
    "        start_id = 0\n",
    "        # the last gradient update may contain less than gradient_interval batches\n",
    "        gradient_interval = min(batch_per_epoch - start_id, solver.gradient_interval)\n",
    "\n",
    "        all_loss = 0\n",
    "        for batch_id, batch in enumerate(islice(dataloader, batch_per_epoch)):\n",
    "            if solver.device.type == \"cuda\":\n",
    "                batch = utils.cuda(batch, device=solver.device)\n",
    "\n",
    "            # loss_1, metric = model(batch)\n",
    "            from torch.nn import functional as F\n",
    "            from torchdrug.layers import functional\n",
    "\n",
    "            all_loss = torch.tensor(0, dtype=torch.float32, device=model.device)\n",
    "            metric = {}\n",
    "\n",
    "            # pred = model.predict(batch, all_loss, metric)\n",
    "            graph = batch[\"graph\"]\n",
    "            if model.graph_construction_model:\n",
    "                graph = model.graph_construction_model(graph)\n",
    "            output = model.model(graph, graph.node_feature.float(), all_loss=all_loss, metric=metric)\n",
    "            pred = model.mlp(output[\"graph_feature\"])\n",
    "            if model.normalization:\n",
    "                pred = pred * model.std + model.mean\n",
    "\n",
    "            # if all([t not in batch for t in self.task]):\n",
    "            #     # unlabeled data\n",
    "            #     return all_loss, metric\n",
    "\n",
    "            target = model.target(batch)\n",
    "            labeled = ~torch.isnan(target)\n",
    "            target[~labeled] = 0\n",
    "\n",
    "            for criterion, weight in model.criterion.items():\n",
    "                loss = F.mse_loss(pred, target, reduction=\"mean\")\n",
    "\n",
    "                name = tasks._get_criterion_name(criterion)\n",
    "                metric[name] = loss\n",
    "            loss_1, metric = loss, metric\n",
    "\n",
    "            # if not loss.requires_grad:\n",
    "            #     raise RuntimeError(\"Loss doesn't require grad. Did you define any loss in the task?\")\n",
    "            loss = loss_1 / gradient_interval\n",
    "            loss.backward()\n",
    "            metrics.append(metric)\n",
    "            all_loss += loss\n",
    "            # print(batch_id, \"Two loss: \", round(loss.item(), 3), round(all_loss.item(), 3))\n",
    "\n",
    "            solver.optimizer.step()\n",
    "            # print(\"optimiser\")\n",
    "            # if batch_id - start_id + 1 == gradient_interval:\n",
    "            #     solver.optimizer.step()\n",
    "            #     solver.optimizer.zero_grad()\n",
    "            #\n",
    "            #     metric = utils.stack(metrics, dim=0)\n",
    "            #     metric = utils.mean(metric, dim=0)\n",
    "            #     if solver.world_size > 1:\n",
    "            #         metric = comm.reduce(metric, op=\"mean\")\n",
    "            #     solver.meter.update(metric)\n",
    "            #\n",
    "            #     metrics = []\n",
    "            #     start_id = batch_id + 1\n",
    "            #     gradient_interval = min(batch_per_epoch - start_id, solver.gradient_interval)\n",
    "        print(\"Loss: \", all_loss)\n",
    "\n",
    "        # if solver.scheduler:\n",
    "        #     # False\n",
    "        #     solver.scheduler.step()\n",
    "    \"\"\"end solver.train(**kwargs)\"\"\"\n",
    "\n",
    "\n",
    "    solver.model.split = \"valid\"\n",
    "    # metric = solver.evaluate(\"valid\")\n",
    "    \"\"\"start metric = solver.evaluate(\"valid\")\"\"\"\n",
    "    split, log = \"valid\", True\n",
    "    if comm.get_rank() == 0:\n",
    "        logger.warning(pretty.separator)\n",
    "        logger.warning(\"Evaluate on %s\" % split)\n",
    "    test_set = getattr(solver, \"%s_set\" % split)\n",
    "    sampler = torch_data.DistributedSampler(test_set, solver.world_size, solver.rank)\n",
    "    dataloader = data.DataLoader(test_set, solver.batch_size, sampler=sampler, num_workers=solver.num_worker)\n",
    "    model = solver.model\n",
    "    model.split = split\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for batch in dataloader:\n",
    "        if solver.device.type == \"cuda\":\n",
    "            batch = utils.cuda(batch, device=solver.device)\n",
    "\n",
    "        pred, target = model.predict_and_target(batch)\n",
    "        preds.append(pred)\n",
    "        targets.append(target)\n",
    "\n",
    "    pred = utils.cat(preds)\n",
    "    target = utils.cat(targets)\n",
    "    if solver.world_size > 1:\n",
    "        pred = comm.cat(pred)\n",
    "        target = comm.cat(target)\n",
    "    metric = model.evaluate(pred, target)\n",
    "    if log:\n",
    "        solver.meter.log(metric, category=\"%s/epoch\" % solver)\n",
    "    solver.batch_size = cfg.engine.batch_size\n",
    "    \"\"\"end metric = solver.evaluate(\"valid\")\"\"\"\n",
    "\n",
    "    score = []\n",
    "    for k, v in metric.items():\n",
    "        if k.startswith(cfg.eval_metric):\n",
    "            if \"root mean squared error\" in cfg.eval_metric:\n",
    "                score.append(-v)\n",
    "            else:\n",
    "                score.append(v)\n",
    "    score = sum(score) / len(score)\n",
    "    if score > best_score:\n",
    "        # print(\"Update best epoch.\")\n",
    "        if best_epoch > -1:\n",
    "            # remove old best epoch model parameters before saving new parameters\n",
    "            to_remove_path = os.path.expanduser(\"model_epoch_%d.pth\" % best_epoch)\n",
    "            # print('removing old parameters at %s' % to_remove_path)\n",
    "            os.remove(to_remove_path)\n",
    "\n",
    "        best_score = score\n",
    "        best_epoch = solver.epoch\n",
    "\n",
    "        # only save epoch model parameters when it is another best epoch\n",
    "        solver.save(\"model_epoch_%d.pth\" % solver.epoch)\n",
    "# clean GPU memory\n",
    "clean_gpu_memory()\n",
    "solver.load(\"model_epoch_%d.pth\" % best_epoch)\n",
    "\n",
    "if comm.get_rank() == 0:\n",
    "    logger.warning(\"Best epoch on valid: %d\" % best_epoch)\n",
    "\"\"\"end solver, best_epoch = train_and_validate(cfg, solver)\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # test(cfg, solver)\n",
    "# \"\"\"start test(cfg, solver)\"\"\"\n",
    "# if \"test_batch_size\" in cfg:\n",
    "#     solver.batch_size = cfg.test_batch_size\n",
    "# solver.model.split = \"valid\"\n",
    "# solver.evaluate(\"valid\")\n",
    "# solver.model.split = \"test\"\n",
    "# solver.evaluate(\"test\")\n",
    "# \"\"\"end test(cfg, solver)\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
