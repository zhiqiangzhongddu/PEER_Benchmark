{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:05.231963Z",
     "start_time": "2023-07-25T12:13:03.175451Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torchdrug.utils import comm, pretty\n",
    "from torchdrug import data, core, utils\n",
    "from torch.utils import data as torch_data\n",
    "\n",
    "from IPython import get_ipython\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "if is_notebook():\n",
    "    sys.path.append('/home/zhiqiang/PEER_Benchmark')\n",
    "else:\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(__file__)))\n",
    "\n",
    "from peer import protbert, util, flip\n",
    "from script.run_single import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:05.238506Z",
     "start_time": "2023-07-25T12:13:05.234487Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# train the model, same as PEER code\n",
    "args = parse_args()\n",
    "\n",
    "args.config = '/home/zhiqiang/PEER_Benchmark/config/single_task/ESM/gb1_ESM_fix.yaml' \\\n",
    "    if is_notebook() else os.path.realpath(args.config)\n",
    "cfg = util.load_config(args.config)\n",
    "if cfg.dataset[\"class\"] != \"Fluorescence\":\n",
    "    cfg.dataset[\"split\"] = args.split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:05.238768Z",
     "start_time": "2023-07-25T12:13:05.236530Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:05   Config file: /home/zhiqiang/PEER_Benchmark/config/single_task/ESM/gb1_ESM_fix.yaml\n",
      "12:13:05   {'dataset': {'atom_feature': None,\n",
      "             'bond_feature': None,\n",
      "             'class': 'GB1',\n",
      "             'path': '~/scratch/protein-datasets/',\n",
      "             'split': 'two_vs_rest',\n",
      "             'transform': {'class': 'Compose',\n",
      "                           'transforms': [{'class': 'ProteinView',\n",
      "                                           'view': 'residue'}]}},\n",
      " 'engine': {'batch_size': 32, 'gpus': [1]},\n",
      " 'eval_metric': 'spearmanr',\n",
      " 'fix_encoder': True,\n",
      " 'optimizer': {'class': 'Adam', 'lr': 5e-05},\n",
      " 'output_dir': '~/scratch/torchprotein_output/',\n",
      " 'task': {'class': 'PropertyPrediction',\n",
      "          'criterion': 'mse',\n",
      "          'metric': ['mae', 'rmse', 'spearmanr'],\n",
      "          'model': {'class': 'ESM',\n",
      "                    'model': 'ESM-1b',\n",
      "                    'path': '~/scratch/protein-model-weights/esm-model-weights/',\n",
      "                    'readout': 'mean'},\n",
      "          'normalization': False,\n",
      "          'num_mlp_layer': 2},\n",
      " 'train': {'num_epoch': 1}}\n",
      "12:13:05   Output dir: /home/zhiqiang/scratch/torchprotein_output/PropertyPrediction/GB1/ESM_2023-07-25-12-13-05\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "output_dir = util.create_working_directory(cfg)\n",
    "logger = util.get_root_logger()\n",
    "if comm.get_rank() == 0:\n",
    "    logger.warning(\"Config file: %s\" % args.config)\n",
    "    logger.warning(pprint.pformat(cfg))\n",
    "    logger.warning(\"Output dir: %s\" % output_dir)\n",
    "    shutil.copyfile(args.config, os.path.basename(args.config))\n",
    "os.chdir(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:05.320153Z",
     "start_time": "2023-07-25T12:13:05.241822Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:05   Extracting /home/zhiqiang/scratch/protein-datasets/gb1/splits.zip to /home/zhiqiang/scratch/protein-datasets/gb1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/zhiqiang/scratch/protein-datasets/gb1/splits/two_vs_rest.csv: 100%|██████████| 8734/8734 [00:00<00:00, 154091.96it/s]\n",
      "Constructing proteins from sequences: 100%|██████████| 8733/8733 [00:06<00:00, 1261.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# solver = build_solver(cfg, logger)\n",
    "# def build_solver(cfg, logger):\n",
    "# build dataset\n",
    "_dataset = core.Configurable.load_config_dict(cfg.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:12.286269Z",
     "start_time": "2023-07-25T12:13:05.264417Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:12.286472Z",
     "start_time": "2023-07-25T12:13:12.286041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:12   GB1(\n",
      "  #sample: 8733\n",
      "  #task: 1\n",
      ")\n",
      "12:13:12   #train: 381, #valid: 43, #test: 8309\n"
     ]
    }
   ],
   "source": [
    "if \"test_split\" in cfg:\n",
    "    train_set, valid_set, test_set = _dataset.split(['train', 'valid', cfg.test_split])\n",
    "else:\n",
    "    train_set, valid_set, test_set = _dataset.split()\n",
    "if comm.get_rank() == 0:\n",
    "    logger.warning(_dataset)\n",
    "    logger.warning(\"#train: %d, #valid: %d, #test: %d\" % (len(train_set), len(valid_set), len(test_set)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:12.286672Z",
     "start_time": "2023-07-25T12:13:12.286206Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:12.286744Z",
     "start_time": "2023-07-25T12:13:12.286427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# build task model\n",
    "if cfg.task[\"class\"] in [\"PropertyPrediction\", \"InteractionPrediction\"]:\n",
    "    cfg.task.task = _dataset.tasks\n",
    "task = core.Configurable.load_config_dict(cfg.task)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:31.288694Z",
     "start_time": "2023-07-25T12:13:12.286525Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "EvolutionaryScaleModeling(\n  (model): ProteinBertModel(\n    (embed_tokens): Embedding(33, 1280, padding_idx=1)\n    (layers): ModuleList(\n      (0): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (12): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (13): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (14): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (15): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (16): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (17): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (18): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (19): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (20): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (21): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (22): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (23): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (24): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (25): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (26): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (27): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (28): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (29): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (30): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (31): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n      (32): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (contact_head): ContactPredictionHead(\n      (regression): Linear(in_features=660, out_features=1, bias=True)\n      (activation): Sigmoid()\n    )\n    (embed_positions): LearnedPositionalEmbedding(1026, 1280, padding_idx=1)\n    (emb_layer_norm_before): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    (lm_head): RobertaLMHead(\n      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (readout): MeanReadout()\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:31.342445Z",
     "start_time": "2023-07-25T12:13:31.293702Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torchdrug.models.esm.EvolutionaryScaleModeling"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(task.model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:31.342701Z",
     "start_time": "2023-07-25T12:13:31.341592Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:31.342761Z",
     "start_time": "2023-07-25T12:13:31.341699Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:31.342810Z",
     "start_time": "2023-07-25T12:13:31.341732Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:31.342861Z",
     "start_time": "2023-07-25T12:13:31.341805Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:31   Preprocess training set\n",
      "12:13:32   {'batch_size': 32,\n",
      " 'class': 'core.Engine',\n",
      " 'gpus': [1],\n",
      " 'gradient_interval': 1,\n",
      " 'log_interval': 100,\n",
      " 'logger': 'logging',\n",
      " 'num_worker': 0,\n",
      " 'optimizer': {'amsgrad': False,\n",
      "               'betas': (0.9, 0.999),\n",
      "               'capturable': False,\n",
      "               'class': 'optim.Adam',\n",
      "               'eps': 1e-08,\n",
      "               'foreach': None,\n",
      "               'lr': 5e-05,\n",
      "               'maximize': False,\n",
      "               'weight_decay': 0},\n",
      " 'scheduler': None,\n",
      " 'task': {'class': 'tasks.PropertyPrediction',\n",
      "          'criterion': 'mse',\n",
      "          'graph_construction_model': None,\n",
      "          'metric': ['mae', 'rmse', 'spearmanr'],\n",
      "          'mlp_batch_norm': False,\n",
      "          'mlp_dropout': 0,\n",
      "          'model': {'class': 'models.ESM',\n",
      "                    'model': 'ESM-1b',\n",
      "                    'path': '~/scratch/protein-model-weights/esm-model-weights/',\n",
      "                    'readout': 'mean'},\n",
      "          'normalization': False,\n",
      "          'num_class': None,\n",
      "          'num_mlp_layer': 2,\n",
      "          'task': ['target'],\n",
      "          'verbose': 0},\n",
      " 'test_set': {'class': 'dataset.Subset',\n",
      "              'dataset': {'atom_feature': None,\n",
      "                          'bond_feature': None,\n",
      "                          'class': 'datasets.GB1',\n",
      "                          'path': '~/scratch/protein-datasets/',\n",
      "                          'split': 'two_vs_rest',\n",
      "                          'transform': {'class': 'transforms.Compose',\n",
      "                                        'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                        'keys': 'graph',\n",
      "                                                        'view': 'residue'}]},\n",
      "                          'verbose': 1},\n",
      "              'indices': range(424, 8733)},\n",
      " 'train_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'datasets.GB1',\n",
      "                           'path': '~/scratch/protein-datasets/',\n",
      "                           'split': 'two_vs_rest',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                         'keys': 'graph',\n",
      "                                                         'view': 'residue'}]},\n",
      "                           'verbose': 1},\n",
      "               'indices': range(0, 381)},\n",
      " 'valid_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'datasets.GB1',\n",
      "                           'path': '~/scratch/protein-datasets/',\n",
      "                           'split': 'two_vs_rest',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [{'class': 'transforms.ProteinView',\n",
      "                                                         'keys': 'graph',\n",
      "                                                         'view': 'residue'}]},\n",
      "                           'verbose': 1},\n",
      "               'indices': range(381, 424)}}\n"
     ]
    }
   ],
   "source": [
    "# fix the pre-trained encoder if specified\n",
    "fix_encoder = cfg.get(\"fix_encoder\", False)\n",
    "fix_encoder2 = cfg.get(\"fix_encoder2\", False)\n",
    "if fix_encoder:\n",
    "    for p in task.model.parameters():\n",
    "        p.requires_grad = False\n",
    "if fix_encoder2:\n",
    "    for p in task.model2.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "# build solver\n",
    "cfg.optimizer.params = task.parameters()\n",
    "optimizer = core.Configurable.load_config_dict(cfg.optimizer)\n",
    "if not \"scheduler\" in cfg:\n",
    "    scheduler = None\n",
    "else:\n",
    "    cfg.scheduler.optimizer = optimizer\n",
    "    scheduler = core.Configurable.load_config_dict(cfg.scheduler)\n",
    "\n",
    "solver = core.Engine(task, train_set, valid_set, test_set, optimizer, scheduler, **cfg.engine)\n",
    "if \"lr_ratio\" in cfg:\n",
    "    cfg.optimizer.params = [\n",
    "        {'params': solver.model.model.parameters(), 'lr': cfg.optimizer.lr * cfg.lr_ratio},\n",
    "        {'params': solver.model.mlp.parameters(), 'lr': cfg.optimizer.lr}\n",
    "    ]\n",
    "    optimizer = core.Configurable.load_config_dict(cfg.optimizer)\n",
    "    solver.optimizer = optimizer\n",
    "if \"checkpoint\" in cfg:\n",
    "    solver.load(cfg.checkpoint, load_optimizer=False)\n",
    "\n",
    "# return solver"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.830597Z",
     "start_time": "2023-07-25T12:13:31.341835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "PropertyPrediction(\n  (model): EvolutionaryScaleModeling(\n    (model): ProteinBertModel(\n      (embed_tokens): Embedding(33, 1280, padding_idx=1)\n      (layers): ModuleList(\n        (0): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (12): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (13): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (14): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (15): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (16): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (17): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (18): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (19): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (20): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (21): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (22): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (23): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (24): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (25): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (26): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (27): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (28): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (29): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (30): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (31): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (32): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (contact_head): ContactPredictionHead(\n        (regression): Linear(in_features=660, out_features=1, bias=True)\n        (activation): Sigmoid()\n      )\n      (embed_positions): LearnedPositionalEmbedding(1026, 1280, padding_idx=1)\n      (emb_layer_norm_before): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (lm_head): RobertaLMHead(\n        (dense): Linear(in_features=1280, out_features=1280, bias=True)\n        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (readout): MeanReadout()\n  )\n  (mlp): MultiLayerPerceptron(\n    (layers): ModuleList(\n      (0): Linear(in_features=1280, out_features=1280, bias=True)\n      (1): Linear(in_features=1280, out_features=1, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.871793Z",
     "start_time": "2023-07-25T12:13:32.833624Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.872094Z",
     "start_time": "2023-07-25T12:13:32.857221Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# solver, best_epoch = train_and_validate(cfg, solver)\n",
    "# def train_and_validate(cfg, solver):\n",
    "step = math.ceil(cfg.train.num_epoch / 10)\n",
    "best_score = float(\"-inf\")\n",
    "best_epoch = -1\n",
    "\n",
    "# if not cfg.train.num_epoch > 0:\n",
    "#     return solver, best_epoch\n",
    "\n",
    "# for i in range(0, cfg.train.num_epoch, step):\n",
    "i = 0\n",
    "kwargs = cfg.train.copy()\n",
    "kwargs[\"num_epoch\"] = min(step, cfg.train.num_epoch - i)\n",
    "solver.model.split = \"train\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.872209Z",
     "start_time": "2023-07-25T12:13:32.857390Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.872374Z",
     "start_time": "2023-07-25T12:13:32.857565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# solver.train(**kwargs)\n",
    "from torch import nn\n",
    "from itertools import islice\n",
    "batch_per_epoch = None\n",
    "num_epoch = kwargs[\"num_epoch\"]\n",
    "\n",
    "sampler = torch_data.DistributedSampler(solver.train_set, solver.world_size, solver.rank)\n",
    "dataloader = data.DataLoader(solver.train_set, solver.batch_size, sampler=sampler, num_workers=solver.num_worker)\n",
    "batch_per_epoch = batch_per_epoch or len(dataloader)\n",
    "model = solver.model\n",
    "if solver.world_size > 1:\n",
    "    if solver.device.type == \"cuda\":\n",
    "        model = nn.parallel.DistributedDataParallel(model, device_ids=[solver.device],\n",
    "                                                    find_unused_parameters=True)\n",
    "    else:\n",
    "        model = nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.873051Z",
     "start_time": "2023-07-25T12:13:32.857670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.873345Z",
     "start_time": "2023-07-25T12:13:32.864571Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "# for epoch in solver.meter(num_epoch):\n",
    "epoch = 0\n",
    "sampler.set_epoch(epoch)\n",
    "\n",
    "metrics = []\n",
    "start_id = 0\n",
    "# the last gradient update may contain less than gradient_interval batches\n",
    "gradient_interval = min(batch_per_epoch - start_id, solver.gradient_interval)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.883717Z",
     "start_time": "2023-07-25T12:13:32.869135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:32.891258Z",
     "start_time": "2023-07-25T12:13:32.883537Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# for batch_id, batch in enumerate(islice(dataloader, batch_per_epoch)):\n",
    "batch_id = 0\n",
    "batch = list(islice(dataloader, batch_per_epoch))[batch_id]\n",
    "if solver.device.type == \"cuda\":\n",
    "    batch = utils.cuda(batch, device=solver.device)\n",
    "\n",
    "# loss, metric = model(batch)\n",
    "from torch.nn import functional as F\n",
    "from torchdrug.layers import functional\n",
    "all_loss = torch.tensor(0, dtype=torch.float32, device=model.device)\n",
    "metric = {}\n",
    "\n",
    "# pred = model.predict(batch, all_loss, metric)\n",
    "graph = batch[\"graph\"]\n",
    "if model.graph_construction_model:\n",
    "    graph = model.graph_construction_model(graph)\n",
    "output = model.model(graph, graph.node_feature.float(), all_loss=all_loss, metric=metric)\n",
    "pred = model.mlp(output[\"graph_feature\"])\n",
    "if model.normalization:\n",
    "    print(model.normalization)\n",
    "    pred = pred * model.std + model.mean"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.362351Z",
     "start_time": "2023-07-25T12:13:32.886911Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0095,  0.1165,  0.0243,  ..., -0.0237, -0.0295,  0.1617],\n       device='cuda:1')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"graph_feature\"][20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.409881Z",
     "start_time": "2023-07-25T12:13:34.364220Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:41:52.890808Z",
     "start_time": "2023-07-25T12:41:52.877893Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2205,  0.0943,  0.2996,  ..., -0.1174, -0.1577, -0.0845],\n        [-0.3352,  0.2010, -0.1284,  ..., -0.1173, -0.0746,  0.3168],\n        [-0.2199,  0.0658,  0.0383,  ..., -0.2070,  0.0414,  0.0626],\n        ...,\n        [-0.0138,  0.4636, -0.0310,  ..., -0.1492, -0.0804, -0.2891],\n        [ 0.0220,  0.2933, -0.0931,  ...,  0.0600, -0.2177, -0.1240],\n        [ 0.2823,  0.3091, -0.0823,  ...,  0.0716, -0.1686,  0.1461]],\n       device='cuda:1')"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"residue_feature\"][:265]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:42:00.175177Z",
     "start_time": "2023-07-25T12:42:00.158681Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "Protein(num_atom=0, num_bond=0, num_residue=265, device='cuda:1')"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:45:47.808346Z",
     "start_time": "2023-07-25T12:45:47.759623Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       device='cuda:1')"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[0].residue_feature[264]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:53:16.746862Z",
     "start_time": "2023-07-25T12:53:16.736601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       device='cuda:1')"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[0].residue_feature[263]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:53:25.711432Z",
     "start_time": "2023-07-25T12:53:25.690517Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.2823,  0.3091, -0.0823,  ...,  0.0716, -0.1686,  0.1461],\n       device='cuda:1')"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"residue_feature\"][264]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:53:32.277191Z",
     "start_time": "2023-07-25T12:53:32.270921Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0220,  0.2933, -0.0931,  ...,  0.0600, -0.2177, -0.1240],\n       device='cuda:1')"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"residue_feature\"][263]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:53:36.863417Z",
     "start_time": "2023-07-25T12:53:36.819202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.2807,  0.3122, -0.0870,  ...,  0.0736, -0.1740,  0.1466],\n       device='cuda:1')"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"residue_feature\"][264+265]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:12:13.680999Z",
     "start_time": "2023-07-25T13:12:13.661275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0187,  0.2989, -0.0911,  ...,  0.0542, -0.2230, -0.1222],\n       device='cuda:1')"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"residue_feature\"][263+265]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:12:14.733520Z",
     "start_time": "2023-07-25T13:12:14.709066Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       device='cuda:1')"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[1].residue_feature[264]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:12:39.008375Z",
     "start_time": "2023-07-25T13:12:38.998554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       device='cuda:1')"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[1].residue_feature[263]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:12:40.188239Z",
     "start_time": "2023-07-25T13:12:40.165778Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "ProteinBertModel(\n  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n  (layers): ModuleList(\n    (0): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (6): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (7): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (8): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (9): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (10): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (11): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (12): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (13): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (14): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (15): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (16): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (17): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (18): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (19): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (20): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (21): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (22): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (23): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (24): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (25): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (26): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (27): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (28): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (29): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (30): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (31): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (32): TransformerLayer(\n      (self_attn): MultiheadAttention(\n        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n      )\n      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (contact_head): ContactPredictionHead(\n    (regression): Linear(in_features=660, out_features=1, bias=True)\n    (activation): Sigmoid()\n  )\n  (embed_positions): LearnedPositionalEmbedding(1026, 1280, padding_idx=1)\n  (emb_layer_norm_before): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.model.model.model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:57:48.699375Z",
     "start_time": "2023-07-25T12:57:48.655768Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(33, 1280, padding_idx=1)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.model.model.model.embed_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:57:11.086728Z",
     "start_time": "2023-07-25T12:57:11.076464Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0093],\n        [0.0094],\n        [0.0075],\n        [0.0094],\n        [0.0091],\n        [0.0072],\n        [0.0080],\n        [0.0085],\n        [0.0083],\n        [0.0096],\n        [0.0078],\n        [0.0080],\n        [0.0080],\n        [0.0099],\n        [0.0103],\n        [0.0093],\n        [0.0108],\n        [0.0100],\n        [0.0075],\n        [0.0082],\n        [0.0086],\n        [0.0096],\n        [0.0090],\n        [0.0090],\n        [0.0069],\n        [0.0085],\n        [0.0092],\n        [0.0092],\n        [0.0082],\n        [0.0065],\n        [0.0057],\n        [0.0101]], device='cuda:1', grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.410140Z",
     "start_time": "2023-07-25T12:13:34.409396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (1): Linear(in_features=1280, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.mlp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.410291Z",
     "start_time": "2023-07-25T12:13:34.409770Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "PropertyPrediction(\n  (model): EvolutionaryScaleModeling(\n    (model): ProteinBertModel(\n      (embed_tokens): Embedding(33, 1280, padding_idx=1)\n      (layers): ModuleList(\n        (0): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (12): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (13): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (14): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (15): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (16): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (17): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (18): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (19): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (20): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (21): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (22): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (23): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (24): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (25): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (26): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (27): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (28): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (29): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (30): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (31): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (32): TransformerLayer(\n          (self_attn): MultiheadAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (contact_head): ContactPredictionHead(\n        (regression): Linear(in_features=660, out_features=1, bias=True)\n        (activation): Sigmoid()\n      )\n      (embed_positions): LearnedPositionalEmbedding(1026, 1280, padding_idx=1)\n      (emb_layer_norm_before): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (lm_head): RobertaLMHead(\n        (dense): Linear(in_features=1280, out_features=1280, bias=True)\n        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (readout): MeanReadout()\n  )\n  (mlp): MultiLayerPerceptron(\n    (layers): ModuleList(\n      (0): Linear(in_features=1280, out_features=1280, bias=True)\n      (1): Linear(in_features=1280, out_features=1, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.480845Z",
     "start_time": "2023-07-25T12:13:34.410022Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "1280"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.output_dim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.481113Z",
     "start_time": "2023-07-25T12:13:34.455183Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[1280, 1]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.model.output_dim] * (model.num_mlp_layer - 1) + [sum(model.num_class)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.519453Z",
     "start_time": "2023-07-25T12:13:34.455273Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mlp_batch_norm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.519655Z",
     "start_time": "2023-07-25T12:13:34.496930Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mlp_dropout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:34.519736Z",
     "start_time": "2023-07-25T12:13:34.497015Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model.train()\n",
    "output = model.model(graph, graph.node_feature.float(), all_loss=all_loss, metric=metric)\n",
    "pred = model.mlp(output[\"graph_feature\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:35.268044Z",
     "start_time": "2023-07-25T12:13:34.497065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0093],\n        [0.0094],\n        [0.0075],\n        [0.0094],\n        [0.0091],\n        [0.0072],\n        [0.0080],\n        [0.0085],\n        [0.0083],\n        [0.0096],\n        [0.0078],\n        [0.0080],\n        [0.0080],\n        [0.0099],\n        [0.0103],\n        [0.0093],\n        [0.0108],\n        [0.0100],\n        [0.0075],\n        [0.0082],\n        [0.0086],\n        [0.0096],\n        [0.0090],\n        [0.0090],\n        [0.0069],\n        [0.0085],\n        [0.0092],\n        [0.0092],\n        [0.0082],\n        [0.0065],\n        [0.0057],\n        [0.0101]], device='cuda:1', grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:35.316487Z",
     "start_time": "2023-07-25T12:13:35.269192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model.model(graph, graph.node_feature.float(), all_loss=all_loss, metric=metric)\n",
    "pred = model.mlp(output[\"graph_feature\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:36.076778Z",
     "start_time": "2023-07-25T12:13:35.308974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0093],\n        [0.0094],\n        [0.0075],\n        [0.0094],\n        [0.0091],\n        [0.0072],\n        [0.0080],\n        [0.0085],\n        [0.0083],\n        [0.0096],\n        [0.0078],\n        [0.0080],\n        [0.0080],\n        [0.0099],\n        [0.0103],\n        [0.0093],\n        [0.0108],\n        [0.0100],\n        [0.0075],\n        [0.0082],\n        [0.0086],\n        [0.0096],\n        [0.0090],\n        [0.0090],\n        [0.0069],\n        [0.0085],\n        [0.0092],\n        [0.0092],\n        [0.0082],\n        [0.0065],\n        [0.0057],\n        [0.0101]], device='cuda:1', grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:36.090651Z",
     "start_time": "2023-07-25T12:13:36.067543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:36.118830Z",
     "start_time": "2023-07-25T12:13:36.074994Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:36.119083Z",
     "start_time": "2023-07-25T12:13:36.090835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:36.119144Z",
     "start_time": "2023-07-25T12:13:36.095300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:36.119197Z",
     "start_time": "2023-07-25T12:13:36.095378Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:36.119249Z",
     "start_time": "2023-07-25T12:13:36.095560Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:13:36.119303Z",
     "start_time": "2023-07-25T12:13:36.095684Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
